# -*- coding: utf-8 -*-
"""Untitled6.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1F-mfx7KCr8p_MKHsD4A2YaTQ5wnZHiYS
"""

import csv
import nltk
from newspaper import Article
from textblob import TextBlob

def calculate_metrics(article_url):
    # Download NLTK resources
    nltk.download('punkt')
    nltk.download('averaged_perceptron_tagger')
    nltk.download('cmudict')

    # Load CMU Pronunciation Dictionary for syllable count
    cmudict_dict = nltk.corpus.cmudict.dict()

    # Initialize counters and variables
    word_count = 0
    sentence_count = 0
    syllable_count = 0
    complex_word_count = 0
    personal_pronoun_count = 0

    # Download and parse the article
    article = Article(article_url)
    article.download()
    article.parse()

    # Get the article text
    text = article.text

    # Tokenize the text into sentences
    sentences = nltk.sent_tokenize(text)
    sentence_count = len(sentences)

    for sentence in sentences:
        # Tokenize each sentence into words
        words = nltk.word_tokenize(sentence)
        word_count += len(words)

        for word in words:
            # Calculate syllable count for each word
            syllables = syllable_count_word(word, cmudict_dict)
            syllable_count += syllables

            # Check for complex words
            if syllables >= 3:
                complex_word_count += 1

            # Check for personal pronouns
            if is_personal_pronoun(word):
                personal_pronoun_count += 1

    # Calculate average sentence length
    avg_sentence_length = word_count / sentence_count

    # Calculate percentage of complex words
    percentage_complex_words = (complex_word_count / word_count) * 100

    # Calculate FOG index
    fog_index = 0.4 * (avg_sentence_length + percentage_complex_words)

    # Calculate average number of words per sentence
    avg_words_per_sentence = word_count / sentence_count

    # Calculate positive and negative scores using TextBlob
    blob = TextBlob(text)
    polarity_score = blob.sentiment.polarity
    subjectivity_score = blob.sentiment.subjectivity

    # Calculate average word length
    avg_word_length = sum(len(word) for word in text.split()) / word_count

    # Calculate syllables per word
    syllable_per_word = syllable_count / word_count

    return {
        "POSITIVE SCORE": polarity_score,
        "NEGATIVE SCORE": -polarity_score,
        "POLARITY SCORE": polarity_score,
        "SUBJECTIVITY SCORE": subjectivity_score,
        "AVG SENTENCE LENGTH": avg_sentence_length,
        "PERCENTAGE OF COMPLEX WORDS": percentage_complex_words,
        "FOG INDEX": fog_index,
        "AVG NUMBER OF WORDS PER SENTENCE": avg_words_per_sentence,
        "COMPLEX WORD COUNT": complex_word_count,
        "WORD COUNT": word_count,
        "SYLLABLE PER WORD": syllable_per_word,
        "PERSONAL PRONOUNS": personal_pronoun_count,
        "AVG WORD LENGTH": avg_word_length
    }

def syllable_count_word(word, cmudict_dict):
    # Calculate syllable count for a given word using the CMU Pronunciation Dictionary
    word = word.lower()
    if word in cmudict_dict:
        return max([len(list(y for y in x if y[-1].isdigit())) for x in cmudict_dict[word]])
    else:
        # Estimate syllable count
        return max(1, len(word) // 2)

def is_personal_pronoun(word):
        # Check if a given word is a personal pronoun
    personal_pronouns = ['i', 'me', 'my', 'mine', 'you', 'your', 'yours',
                         'he', 'him', 'his', 'she', 'her', 'hers',
                         'we', 'us', 'our', 'ours', 'they', 'them', 'their', 'theirs']
    return word.lower() in personal_pronouns

# Read URLs from CSV file
def read_urls_from_csv(file_path):
    urls = []
    with open(file_path, 'r') as csv_file:
        reader = csv.reader(csv_file)
        next(reader)  # Skip header row
        for row in reader:
            url = row[0]
            urls.append(url)
    return urls

# Process URLs and calculate metrics
def process_urls(file_path):
    # Download NLTK resources
    nltk.download('punkt')
    nltk.download('averaged_perceptron_tagger')
    nltk.download('cmudict')

    # Read URLs from CSV file
    urls = read_urls_from_csv(file_path)

    # Iterate through each URL
    for url in urls:
        try:
            # Calculate metrics for each URL
            metrics = calculate_metrics(url)

            # Print the calculated metrics
            print(f"Metrics for URL: {url}")
            for metric, value in metrics.items():
                print(f"{metric}: {value}")
            print("-----------------------")
        except Exception as e:
            print(f"Error processing URL: {url}")
            print(e)
            print("-----------------------")

# Specify the path to the CSV file containing the URLs
csv_file_path = "/content/Input.csv"

# Process the URLs and calculate metrics
process_urls(csv_file_path)